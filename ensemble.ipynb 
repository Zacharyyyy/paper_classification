{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Activation, Conv1D, Dense, Embedding, Flatten, Input, MaxPooling1D, Dropout, Input, Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.metrics import categorical_accuracy, binary_accuracy\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from collections import Counter\n",
    "import gc\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, BatchNormalization\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(pd.read_csv('data.csv', encoding = \"utf8\"))\n",
    "#data = pd.read_csv('data.csv', encoding = \"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "      <th>THEORETICAL</th>\n",
       "      <th>ENGINEERING</th>\n",
       "      <th>EMPIRICAL</th>\n",
       "      <th>OTHERS</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rapid popularity of Internet of Things (IoT) a...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A Brain-Inspired Trust Management Model to Ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In this paper, we address the problem of compu...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>On Efficient Computation of Shortest Dubins Pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>High quality upsampling of sparse 3D point clo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Data-driven Upsampling of Point Clouds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Internet is the main source of information now...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Accessibility or Usability of InteractSE? A He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Automated Facial Expression Recognition (FER) ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Spatio-Temporal Facial Expression Recognition ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Abstract  THEORETICAL  \\\n",
       "0  Rapid popularity of Internet of Things (IoT) a...          1.0   \n",
       "1  In this paper, we address the problem of compu...          1.0   \n",
       "2  High quality upsampling of sparse 3D point clo...          0.0   \n",
       "3  Internet is the main source of information now...          0.0   \n",
       "4  Automated Facial Expression Recognition (FER) ...          0.0   \n",
       "\n",
       "   ENGINEERING  EMPIRICAL  OTHERS  \\\n",
       "0          0.0        0.0     0.0   \n",
       "1          0.0        0.0     0.0   \n",
       "2          1.0        0.0     0.0   \n",
       "3          0.0        1.0     0.0   \n",
       "4          1.0        0.0     0.0   \n",
       "\n",
       "                                               Title  \n",
       "0  A Brain-Inspired Trust Management Model to Ass...  \n",
       "1  On Efficient Computation of Shortest Dubins Pa...  \n",
       "2             Data-driven Upsampling of Point Clouds  \n",
       "3  Accessibility or Usability of InteractSE? A He...  \n",
       "4  Spatio-Temporal Facial Expression Recognition ...  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['Abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rapid popularity of Internet of Things (IoT) and cloud computing permits neuroscientists to collect multilevel and multichannel brain data to better understand brain functions, diagnose diseases, and devise treatments.$$$To ensure secure and reliable data communication between end-to-end (E2E) devices supported by current IoT and cloud infrastructure, trust management is needed at the IoT and user ends.$$$This paper introduces a Neuro-Fuzzy based Brain-inspired trust management model (TMM) to secure IoT devices and relay nodes, and to ensure data reliability.$$$The proposed TMM utilizes node behavioral trust and data trust estimated using Adaptive Neuro-Fuzzy Inference System and weighted-additive methods respectively to assess the nodes trustworthiness.$$$In contrast to the existing fuzzy based TMMs, the NS2 simulation results confirm the robustness and accuracy of the proposed TMM in identifying malicious nodes in the communication network.$$$With the growing usage of cloud based IoT frameworks in Neuroscience research, integrating the proposed TMM into the existing infrastructure will assure secure and reliable data communication among the E2E devices.'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rapid popularity internet things ( iot ) cloud computing permits neuroscientists collect multilevel multichannel brain data better understand brain functions , diagnose diseases , devise treatments.\n",
      "ensure secure reliable data communication end-to-end ( e2e ) devices supported current iot cloud infrastructure , trust management needed iot user ends.\n",
      "paper introduces neuro-fuzzy based brain-inspired trust management model ( tmm ) secure iot devices relay nodes , ensure data reliability.\n",
      "proposed tmm utilizes node behavioral trust data trust estimated using adaptive neuro-fuzzy inference system weighted-additive methods respectively assess nodes trustworthiness.\n",
      "contrast existing fuzzy based tmms , ns2 simulation results confirm robustness accuracy proposed tmm identifying malicious nodes communication network.\n",
      "growing usage cloud based iot frameworks neuroscience research , integrating proposed tmm existing infrastructure assure secure reliable data communication among e2e devices .\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = texts.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abstract = abstract.str.lower()\n",
    "for i in range(len(texts)):\n",
    "  str = texts[i]\n",
    "  texts[i] = str.replace('$$$', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "for c in string.punctuation:\n",
    "    texts = texts.str.replace(c, ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english'))\n",
    "for i in range(len(texts)):\n",
    "    token = word_tokenize(texts[i]) \n",
    "    temp = [w for w in token if not w in stop_words]\n",
    "    texts[i] = \" \".join(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for i in range(len(texts)):\n",
    "    input_str = word_tokenize(texts[i])\n",
    "    for j in range(len(input_str)):\n",
    "        input_str[j] = lemmatizer.lemmatize(input_str[j])\n",
    "    texts[i] = \" \".join(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for i in range(len(texts)):\n",
    "    texts[i] = re.sub(r'\\b\\d+\\b *|\\b[a-z]\\b *', '', texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "title = data[\"Title\"][:7000]\n",
    "\n",
    "title = title.str.lower()\n",
    "\n",
    "for c in string.punctuation:\n",
    "    title = title.str.replace(c, ' ')\n",
    "    \n",
    "stop_words = set(stopwords.words('english'))\n",
    "for i in range(len(title)):\n",
    "    token = word_tokenize(title[i]) \n",
    "    temp = [w for w in token if not w in stop_words]\n",
    "    title[i] = \" \".join(temp)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for i in range(len(title)):\n",
    "    input_str = word_tokenize(title[i])\n",
    "    for j in range(len(input_str)):\n",
    "        input_str[j] = lemmatizer.lemmatize(input_str[j])\n",
    "    title[i] = \" \".join(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('network', 502),\n",
       " ('learning', 309),\n",
       " ('based', 281),\n",
       " ('system', 216),\n",
       " ('model', 209),\n",
       " ('using', 187),\n",
       " ('analysis', 168),\n",
       " ('algorithm', 137),\n",
       " ('multi', 136),\n",
       " ('data', 135)]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theory = title.loc[data['THEORETICAL'] == 1]\n",
    "word_tokens = word_tokenize(\" \".join(theory.tolist())) \n",
    "Counter(word_tokens).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('network', 556),\n",
       " ('based', 429),\n",
       " ('using', 417),\n",
       " ('learning', 407),\n",
       " ('system', 274),\n",
       " ('deep', 256),\n",
       " ('neural', 246),\n",
       " ('image', 226),\n",
       " ('model', 217),\n",
       " ('data', 203)]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theory = title.loc[data['ENGINEERING'] == 1]\n",
    "word_tokens = word_tokenize(\" \".join(theory.tolist())) \n",
    "Counter(word_tokens).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('network', 2741),\n",
       " ('model', 2569),\n",
       " ('algorithm', 2314),\n",
       " ('problem', 2216),\n",
       " ('method', 2062),\n",
       " ('based', 1810),\n",
       " ('data', 1809),\n",
       " ('system', 1747),\n",
       " ('paper', 1644),\n",
       " ('result', 1591),\n",
       " ('show', 1453),\n",
       " ('approach', 1402),\n",
       " ('time', 1383),\n",
       " ('learning', 1317),\n",
       " ('proposed', 1234),\n",
       " ('using', 1163),\n",
       " ('information', 1112),\n",
       " ('two', 1102),\n",
       " ('also', 1056),\n",
       " ('one', 997)]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "theory = texts.loc[data['THEORETICAL'] == 1]\n",
    "word_tokens = word_tokenize(\" \".join(theory.tolist())) \n",
    "Counter(word_tokens).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('network', 2912),\n",
       " ('model', 2890),\n",
       " ('data', 2697),\n",
       " ('method', 2599),\n",
       " ('based', 2421),\n",
       " ('system', 2242),\n",
       " ('algorithm', 2068),\n",
       " ('paper', 1964),\n",
       " ('image', 1893),\n",
       " ('approach', 1821),\n",
       " ('using', 1799),\n",
       " ('result', 1677),\n",
       " ('problem', 1649),\n",
       " ('learning', 1610),\n",
       " ('proposed', 1608),\n",
       " ('performance', 1483),\n",
       " ('time', 1471),\n",
       " ('show', 1379),\n",
       " ('propose', 1160),\n",
       " ('feature', 1130)]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engineer = texts.loc[data['ENGINEERING'] == 1]\n",
    "word_tokens = word_tokenize(\" \".join(engineer.tolist())) \n",
    "Counter(word_tokens).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sentences = []\n",
    "for i in range(len(texts)):\n",
    "    sentences.append(\" \".join(word_tokenize(texts[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 61477\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size) # Setup tokenizer\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts) # Generate sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50,849 unique words.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found {:,} unique words.'.format(len(word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98.95948148148148, 36.944325316135306)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the average length of a text\n",
    "avg = sum(map(len, sequences)) / len(sequences)\n",
    "\n",
    "# Get the standard deviation of the sequence length\n",
    "std = np.sqrt(sum(map(lambda x: (len(x) - avg)**2, sequences)) / len(sequences))\n",
    "\n",
    "avg,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 140\n",
    "abstract = pad_sequences(sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sentences = []\n",
    "for i in range(len(texts)):\n",
    "    sentences.append(word_tokenize(texts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lc = LancasterStemmer()\n",
    "from nltk.stem import SnowballStemmer\n",
    "sb = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27000it [00:33, 794.16it/s]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "text_list = texts\n",
    "nlp = spacy.load('en_core_web_lg', disable=['parser','ner','tagger'])\n",
    "nlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)\n",
    "word_dict = {}\n",
    "word_index = 1\n",
    "lemma_dict = {}\n",
    "docs = nlp.pipe(text_list, n_threads = 2)\n",
    "word_sequences = []\n",
    "for doc in tqdm(docs):\n",
    "    word_seq = []\n",
    "    for token in doc:\n",
    "        if (token.text not in word_dict) and (token.pos_ is not \"PUNCT\"):\n",
    "            word_dict[token.text] = word_index\n",
    "            word_index += 1\n",
    "            lemma_dict[token.text] = token.lemma_\n",
    "        if token.pos_ is not \"PUNCT\":\n",
    "            word_seq.append(word_dict[token.text])\n",
    "    word_sequences.append(word_seq)\n",
    "del docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fasttext(word_dict, lemma_dict):\n",
    "    EMBEDDING_FILE = 'input/fasttext/wiki-news-300d-1M.vec'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
    "    embed_size = 300\n",
    "    nb_words = len(word_dict)+1\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32)\n",
    "    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1.\n",
    "    print(unknown_vector[:5])\n",
    "    for key in tqdm(word_dict):\n",
    "        word = key\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.lower()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.upper()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.capitalize()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = ps.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lc.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = sb.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lemma_dict[key]\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        \"\"\"\n",
    "        if len(key) > 1:\n",
    "            word = correction(key)\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[word_dict[key]] = embedding_vector\n",
    "                continue\n",
    "        \"\"\"\n",
    "        embedding_matrix[word_dict[key]] = unknown_vector                    \n",
    "    return embedding_matrix, nb_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(word_dict, lemma_dict):\n",
    "    EMBEDDING_FILE = 'glove.6B/glove.6B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "    embed_size = 300\n",
    "    nb_words = len(word_dict)+1\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32)\n",
    "    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1.\n",
    "    print(unknown_vector[:5])\n",
    "    for key in tqdm(word_dict):\n",
    "        word = key\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.lower()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.upper()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.capitalize()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = ps.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lc.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = sb.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lemma_dict[key]\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        \"\"\"\n",
    "        if len(key) > 1:\n",
    "            word = correction(key)\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[word_dict[key]] = embedding_vector\n",
    "                continue\n",
    "        \"\"\"\n",
    "        embedding_matrix[word_dict[key]] = unknown_vector                    \n",
    "    return embedding_matrix, nb_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3506/143783 [00:00<00:04, 35049.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. -1. -1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143783/143783 [00:07<00:00, 18036.79it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_fasttext, nb_words = load_fasttext(word_dict, lemma_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 8854/143783 [00:00<00:02, 46163.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. -1. -1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143783/143783 [00:07<00:00, 18772.72it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_glove, nb_words = load_glove(word_dict, lemma_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((143784, 300), (143784, 300), (143784, 600))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.concatenate((embedding_matrix_glove, embedding_matrix_fasttext), axis=1)\n",
    "embedding_matrix_glove.shape, embedding_matrix_fasttext.shape, embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM = 300\n",
    "\n",
    "w2v = Word2Vec(sentences, size = EMB_DIM, window = 5, min_count = 3, negative = 15, iter = 10, workers = multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector = w2v.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400,000 word vectors in GloVe.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {} # We create a dictionary of word -> embedding\n",
    "\n",
    "with open('glove.6B/glove.6B.300d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0] # The first value is the word, the rest are the values of the embedding\n",
    "        embedding = np.asarray(values[1:], dtype='float32') # Load embedding\n",
    "        embeddings_index[word] = embedding # Add embedding to our embedding dictionary\n",
    "\n",
    "print('Found {:,} word vectors in GloVe.'.format(len(embeddings_index)))\n",
    "\n",
    "embedding_dim = 300 # We use 100 dimensional glove vectors\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(vocab_size, len(word_index)) # How many words are there actually\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "\n",
    "# The vectors need to be in the same position as their index. \n",
    "# Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on\n",
    "not_find = 0\n",
    "# Loop over all words in the word index\n",
    "for word, i in word_index.items():\n",
    "    # If we are above the amount of words we want to use we do nothing\n",
    "    if i >= vocab_size: \n",
    "        continue\n",
    "    # Get the embedding vector for the word\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    # If there is an embedding vector, put it in the embedding matrix\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        not_find += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300 # We use 100 dimensional glove vectors\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(vocab_size, len(word_index)) # How many words are there actually\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "\n",
    "# The vectors need to be in the same position as their index. \n",
    "# Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on\n",
    "not_find = 0\n",
    "# Loop over all words in the word index\n",
    "for word, i in word_index.items():\n",
    "    # If we are above the amount of words we want to use we do nothing\n",
    "    if i >= vocab_size: \n",
    "        continue\n",
    "    # Get the embedding vector for the word\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    # If there is an embedding vector, put it in the embedding matrix\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        not_find += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18712"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15967"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16,674 word vectors in w2v.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {} # We create a dictionary of word -> embedding\n",
    "\"\"\"\n",
    "with open('input/fasttext/wiki-news-300d-1M.vec') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0] # The first value is the word, the rest are the values of the embedding\n",
    "        embedding = np.asarray(values[1:], dtype='float32') # Load embedding\n",
    "        embeddings_index[word] = embedding # Add embedding to our embedding dictionary\n",
    "\"\"\"\n",
    "print('Found {:,} word vectors in w2v.'.format(len(word_vector.index2word)))\n",
    "\n",
    "embedding_dim = 300 # We use 100 dimensional glove vectors\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(vocab_size, len(word_index)) # How many words are there actually\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "\n",
    "# The vectors need to be in the same position as their index. \n",
    "# Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on\n",
    "not_find = 0\n",
    "# Loop over all words in the word index\n",
    "for word, i in word_index.items():\n",
    "    # If we are above the amount of words we want to use we do nothing\n",
    "    if i >= vocab_size: \n",
    "        continue\n",
    "    # Get the embedding vector for the word\n",
    "    if word in word_vector.index2word:\n",
    "        embedding_vector = word_vector[word]\n",
    "    else:\n",
    "        not_find += 1\n",
    "        continue\n",
    "    # If there is an embedding vector, put it in the embedding matrix\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18064"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_find"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model1(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
    "    file_path = \"best_model.h5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                                  save_best_only = True, mode = \"min\")\n",
    "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 5)\n",
    "    \n",
    "    inp = Input(shape = (max_length,))\n",
    "    x = Embedding(vocab_size, embedding_dim, input_length=max_length, weights = [embedding_matrix], trainable = True)(inp)\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "\n",
    "    x_gru = Bidirectional(GRU(units, return_sequences = True))(x1)\n",
    "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool1_gru = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1_gru = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool3_gru = GlobalAveragePooling1D()(x3)\n",
    "    max_pool3_gru = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "    x_lstm = Bidirectional(LSTM(units, return_sequences = True))(x1)\n",
    "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1_lstm = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n",
    "    max_pool3_lstm = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "    \n",
    "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n",
    "                    avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "    x = Dense(4, activation = \"sigmoid\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = ['accuracy'])\n",
    "    model.fit(abstract[:6000], np.array(data[:6000][['THEORETICAL','ENGINEERING','EMPIRICAL','OTHERS']]), batch_size = 128, epochs = 200, shuffle = True, validation_split=0.1, verbose = 1, callbacks = [check_point, early_stop])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5400 samples, validate on 600 samples\n",
      "Epoch 1/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.5792 - accuracy: 0.6950\n",
      "Epoch 00001: val_loss improved from inf to 0.74890, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 301s 56ms/sample - loss: 0.5790 - accuracy: 0.6951 - val_loss: 0.7489 - val_accuracy: 0.6675\n",
      "Epoch 2/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4755 - accuracy: 0.7692\n",
      "Epoch 00002: val_loss improved from 0.74890 to 0.49386, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 294s 55ms/sample - loss: 0.4757 - accuracy: 0.7690 - val_loss: 0.4939 - val_accuracy: 0.7692\n",
      "Epoch 3/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4463 - accuracy: 0.7871\n",
      "Epoch 00003: val_loss did not improve from 0.49386\n",
      "5400/5400 [==============================] - 287s 53ms/sample - loss: 0.4462 - accuracy: 0.7871 - val_loss: 0.4982 - val_accuracy: 0.7658\n",
      "Epoch 4/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4327 - accuracy: 0.7952\n",
      "Epoch 00004: val_loss improved from 0.49386 to 0.46904, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 283s 52ms/sample - loss: 0.4328 - accuracy: 0.7952 - val_loss: 0.4690 - val_accuracy: 0.7750\n",
      "Epoch 5/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4156 - accuracy: 0.8032\n",
      "Epoch 00005: val_loss did not improve from 0.46904\n",
      "5400/5400 [==============================] - 281s 52ms/sample - loss: 0.4155 - accuracy: 0.8032 - val_loss: 0.4831 - val_accuracy: 0.7746\n",
      "Epoch 6/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.3992 - accuracy: 0.8154\n",
      "Epoch 00006: val_loss improved from 0.46904 to 0.45253, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 283s 52ms/sample - loss: 0.3990 - accuracy: 0.8156 - val_loss: 0.4525 - val_accuracy: 0.7892\n",
      "Epoch 7/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.3756 - accuracy: 0.8265\n",
      "Epoch 00007: val_loss did not improve from 0.45253\n",
      "5400/5400 [==============================] - 300s 56ms/sample - loss: 0.3760 - accuracy: 0.8263 - val_loss: 0.4780 - val_accuracy: 0.7742\n",
      "Epoch 8/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.3602 - accuracy: 0.8358\n",
      "Epoch 00008: val_loss did not improve from 0.45253\n",
      "5400/5400 [==============================] - 295s 55ms/sample - loss: 0.3599 - accuracy: 0.8359 - val_loss: 0.4809 - val_accuracy: 0.7812\n",
      "Epoch 9/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.3328 - accuracy: 0.8540\n",
      "Epoch 00009: val_loss did not improve from 0.45253\n",
      "5400/5400 [==============================] - 296s 55ms/sample - loss: 0.3326 - accuracy: 0.8539 - val_loss: 0.4677 - val_accuracy: 0.7738\n",
      "Epoch 10/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.3105 - accuracy: 0.8646\n",
      "Epoch 00010: val_loss did not improve from 0.45253\n",
      "5400/5400 [==============================] - 295s 55ms/sample - loss: 0.3105 - accuracy: 0.8647 - val_loss: 0.5812 - val_accuracy: 0.7588\n",
      "Epoch 11/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.2786 - accuracy: 0.8810\n",
      "Epoch 00011: val_loss did not improve from 0.45253\n",
      "5400/5400 [==============================] - 294s 55ms/sample - loss: 0.2786 - accuracy: 0.8810 - val_loss: 0.4826 - val_accuracy: 0.7663\n"
     ]
    }
   ],
   "source": [
    "model1 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 512, spatial_dr = 0.3, kernel_size1=3, kernel_size2=2, dense_units=32, dr=0.1, conv_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5400 samples, validate on 600 samples\n",
      "Epoch 1/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.6025 - accuracy: 0.6746\n",
      "Epoch 00001: val_loss improved from inf to 0.49517, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 343s 64ms/sample - loss: 0.6021 - accuracy: 0.6750 - val_loss: 0.4952 - val_accuracy: 0.7621\n",
      "Epoch 2/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4894 - accuracy: 0.7654\n",
      "Epoch 00002: val_loss improved from 0.49517 to 0.47853, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 333s 62ms/sample - loss: 0.4895 - accuracy: 0.7654 - val_loss: 0.4785 - val_accuracy: 0.7746\n",
      "Epoch 3/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4654 - accuracy: 0.7746\n",
      "Epoch 00003: val_loss improved from 0.47853 to 0.47591, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 321s 59ms/sample - loss: 0.4652 - accuracy: 0.7748 - val_loss: 0.4759 - val_accuracy: 0.7708\n",
      "Epoch 4/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4478 - accuracy: 0.7838\n",
      "Epoch 00004: val_loss improved from 0.47591 to 0.46998, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 308s 57ms/sample - loss: 0.4481 - accuracy: 0.7835 - val_loss: 0.4700 - val_accuracy: 0.7742\n",
      "Epoch 5/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4347 - accuracy: 0.7916\n",
      "Epoch 00005: val_loss did not improve from 0.46998\n",
      "5400/5400 [==============================] - 324s 60ms/sample - loss: 0.4350 - accuracy: 0.7913 - val_loss: 0.4840 - val_accuracy: 0.7663\n",
      "Epoch 6/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4222 - accuracy: 0.7989\n",
      "Epoch 00006: val_loss improved from 0.46998 to 0.44538, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 324s 60ms/sample - loss: 0.4226 - accuracy: 0.7988 - val_loss: 0.4454 - val_accuracy: 0.7821\n",
      "Epoch 7/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4148 - accuracy: 0.8039\n",
      "Epoch 00007: val_loss improved from 0.44538 to 0.43918, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 307s 57ms/sample - loss: 0.4146 - accuracy: 0.8040 - val_loss: 0.4392 - val_accuracy: 0.7804\n",
      "Epoch 8/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4056 - accuracy: 0.8116\n",
      "Epoch 00008: val_loss improved from 0.43918 to 0.43492, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 300s 56ms/sample - loss: 0.4054 - accuracy: 0.8117 - val_loss: 0.4349 - val_accuracy: 0.7833\n",
      "Epoch 9/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.3999 - accuracy: 0.8118\n",
      "Epoch 00009: val_loss did not improve from 0.43492\n",
      "5400/5400 [==============================] - 296s 55ms/sample - loss: 0.3999 - accuracy: 0.8120 - val_loss: 0.4463 - val_accuracy: 0.7804\n",
      "Epoch 10/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.3923 - accuracy: 0.8179\n",
      "Epoch 00010: val_loss did not improve from 0.43492\n",
      "5400/5400 [==============================] - 321s 59ms/sample - loss: 0.3923 - accuracy: 0.8180 - val_loss: 0.4433 - val_accuracy: 0.7833\n",
      "Epoch 11/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.3754 - accuracy: 0.8280\n",
      "Epoch 00011: val_loss did not improve from 0.43492\n",
      "5400/5400 [==============================] - 291s 54ms/sample - loss: 0.3756 - accuracy: 0.8279 - val_loss: 0.5011 - val_accuracy: 0.7492\n",
      "Epoch 12/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.3617 - accuracy: 0.8359\n",
      "Epoch 00012: val_loss did not improve from 0.43492\n",
      "5400/5400 [==============================] - 296s 55ms/sample - loss: 0.3616 - accuracy: 0.8360 - val_loss: 0.4601 - val_accuracy: 0.7821\n",
      "Epoch 13/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.3501 - accuracy: 0.8403\n",
      "Epoch 00013: val_loss did not improve from 0.43492\n",
      "5400/5400 [==============================] - 285s 53ms/sample - loss: 0.3503 - accuracy: 0.8399 - val_loss: 0.4669 - val_accuracy: 0.7750\n"
     ]
    }
   ],
   "source": [
    "model2 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 512, spatial_dr = 0.5, kernel_size1=3, kernel_size2=2, dense_units=64, dr=0.2, conv_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model2(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
    "    file_path = \"best_model.h5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                                  save_best_only = True, mode = \"min\")\n",
    "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 10)\n",
    "\n",
    "    inp = Input(shape = (max_length,))\n",
    "    x = Embedding(vocab_size, embedding_dim, input_length=max_length, weights = [embedding_matrix], trainable = True)(inp)\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "\n",
    "    x_gru = Bidirectional(GRU(units, return_sequences = True))(x1)\n",
    "    x_lstm = Bidirectional(LSTM(units, return_sequences = True))(x1)\n",
    "    \n",
    "    x_conv1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool1_gru = GlobalAveragePooling1D()(x_conv1)\n",
    "    max_pool1_gru = GlobalMaxPooling1D()(x_conv1)\n",
    "    \n",
    "    x_conv2 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool2_gru = GlobalAveragePooling1D()(x_conv2)\n",
    "    max_pool2_gru = GlobalMaxPooling1D()(x_conv2)\n",
    "    \n",
    "    x_conv3 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool1_lstm = GlobalAveragePooling1D()(x_conv3)\n",
    "    max_pool1_lstm = GlobalMaxPooling1D()(x_conv3)\n",
    "    \n",
    "    x_conv4 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool2_lstm = GlobalAveragePooling1D()(x_conv4)\n",
    "    max_pool2_lstm = GlobalMaxPooling1D()(x_conv4)\n",
    "    \n",
    "    \n",
    "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool2_gru, max_pool2_gru,\n",
    "                    avg_pool1_lstm, max_pool1_lstm, avg_pool2_lstm, max_pool2_lstm])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(int(dense_units / 4), activation='relu') (x))\n",
    "    x = Dense(4, activation = \"sigmoid\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = ['accuracy'])\n",
    "    model.fit(abstract[:6000], np.array(data[:6000][['THEORETICAL','ENGINEERING','EMPIRICAL','OTHERS']]), batch_size = 128, epochs = 200, shuffle = True, validation_split=0.1, verbose = 1, callbacks = [check_point, early_stop])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5400 samples, validate on 600 samples\n",
      "Epoch 1/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.7092 - accuracy: 0.5920 \n",
      "Epoch 00001: val_loss improved from inf to 0.66498, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 378s 70ms/sample - loss: 0.7093 - accuracy: 0.5921 - val_loss: 0.6650 - val_accuracy: 0.6363\n",
      "Epoch 2/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.6529 - accuracy: 0.6338\n",
      "Epoch 00002: val_loss improved from 0.66498 to 0.64911, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 366s 68ms/sample - loss: 0.6528 - accuracy: 0.6338 - val_loss: 0.6491 - val_accuracy: 0.6321\n",
      "Epoch 3/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.6287 - accuracy: 0.6565 \n",
      "Epoch 00003: val_loss improved from 0.64911 to 0.62673, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 368s 68ms/sample - loss: 0.6287 - accuracy: 0.6564 - val_loss: 0.6267 - val_accuracy: 0.6592\n",
      "Epoch 4/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.6117 - accuracy: 0.6672\n",
      "Epoch 00004: val_loss improved from 0.62673 to 0.60469, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 341s 63ms/sample - loss: 0.6115 - accuracy: 0.6674 - val_loss: 0.6047 - val_accuracy: 0.6917\n",
      "Epoch 5/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.6001 - accuracy: 0.6788\n",
      "Epoch 00005: val_loss improved from 0.60469 to 0.58592, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 333s 62ms/sample - loss: 0.6004 - accuracy: 0.6784 - val_loss: 0.5859 - val_accuracy: 0.7071\n",
      "Epoch 6/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.5907 - accuracy: 0.6894\n",
      "Epoch 00006: val_loss improved from 0.58592 to 0.57223, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 330s 61ms/sample - loss: 0.5906 - accuracy: 0.6895 - val_loss: 0.5722 - val_accuracy: 0.7196\n",
      "Epoch 7/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.5789 - accuracy: 0.7043\n",
      "Epoch 00007: val_loss improved from 0.57223 to 0.55966, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 329s 61ms/sample - loss: 0.5787 - accuracy: 0.7046 - val_loss: 0.5597 - val_accuracy: 0.7287\n",
      "Epoch 8/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.5766 - accuracy: 0.7064\n",
      "Epoch 00008: val_loss improved from 0.55966 to 0.55027, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 331s 61ms/sample - loss: 0.5764 - accuracy: 0.7066 - val_loss: 0.5503 - val_accuracy: 0.7429\n",
      "Epoch 9/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.5651 - accuracy: 0.7139\n",
      "Epoch 00009: val_loss improved from 0.55027 to 0.54550, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 329s 61ms/sample - loss: 0.5652 - accuracy: 0.7140 - val_loss: 0.5455 - val_accuracy: 0.7442\n",
      "Epoch 10/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.5588 - accuracy: 0.7257\n",
      "Epoch 00010: val_loss improved from 0.54550 to 0.53798, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 330s 61ms/sample - loss: 0.5585 - accuracy: 0.7260 - val_loss: 0.5380 - val_accuracy: 0.7571\n",
      "Epoch 11/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.5556 - accuracy: 0.7267\n",
      "Epoch 00011: val_loss improved from 0.53798 to 0.53490, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 330s 61ms/sample - loss: 0.5555 - accuracy: 0.7267 - val_loss: 0.5349 - val_accuracy: 0.7550\n",
      "Epoch 12/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.5498 - accuracy: 0.7305\n",
      "Epoch 00012: val_loss improved from 0.53490 to 0.53041, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 330s 61ms/sample - loss: 0.5495 - accuracy: 0.7307 - val_loss: 0.5304 - val_accuracy: 0.7642\n",
      "Epoch 13/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.5413 - accuracy: 0.7396\n",
      "Epoch 00013: val_loss improved from 0.53041 to 0.52529, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 328s 61ms/sample - loss: 0.5412 - accuracy: 0.7395 - val_loss: 0.5253 - val_accuracy: 0.7671\n",
      "Epoch 14/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.5379 - accuracy: 0.7414\n",
      "Epoch 00014: val_loss improved from 0.52529 to 0.52163, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 329s 61ms/sample - loss: 0.5382 - accuracy: 0.7411 - val_loss: 0.5216 - val_accuracy: 0.7675\n",
      "Epoch 15/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.5336 - accuracy: 0.7420\n",
      "Epoch 00015: val_loss improved from 0.52163 to 0.51582, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 329s 61ms/sample - loss: 0.5337 - accuracy: 0.7417 - val_loss: 0.5158 - val_accuracy: 0.7671\n",
      "Epoch 16/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.5305 - accuracy: 0.7450\n",
      "Epoch 00016: val_loss improved from 0.51582 to 0.51406, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 337s 62ms/sample - loss: 0.5306 - accuracy: 0.7450 - val_loss: 0.5141 - val_accuracy: 0.7721\n",
      "Epoch 17/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.5232 - accuracy: 0.7513\n",
      "Epoch 00017: val_loss improved from 0.51406 to 0.51336, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 348s 64ms/sample - loss: 0.5228 - accuracy: 0.7514 - val_loss: 0.5134 - val_accuracy: 0.7725\n",
      "Epoch 18/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.5236 - accuracy: 0.7515\n",
      "Epoch 00018: val_loss improved from 0.51336 to 0.50814, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 332s 61ms/sample - loss: 0.5232 - accuracy: 0.7518 - val_loss: 0.5081 - val_accuracy: 0.7746\n",
      "Epoch 19/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.5141 - accuracy: 0.7555\n",
      "Epoch 00019: val_loss improved from 0.50814 to 0.50523, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 331s 61ms/sample - loss: 0.5139 - accuracy: 0.7556 - val_loss: 0.5052 - val_accuracy: 0.7700\n",
      "Epoch 20/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.5097 - accuracy: 0.7604\n",
      "Epoch 00020: val_loss improved from 0.50523 to 0.50441, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 333s 62ms/sample - loss: 0.5097 - accuracy: 0.7605 - val_loss: 0.5044 - val_accuracy: 0.7708\n",
      "Epoch 21/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.5100 - accuracy: 0.7600\n",
      "Epoch 00021: val_loss improved from 0.50441 to 0.50318, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 330s 61ms/sample - loss: 0.5098 - accuracy: 0.7602 - val_loss: 0.5032 - val_accuracy: 0.7692\n",
      "Epoch 22/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.5071 - accuracy: 0.7591\n",
      "Epoch 00022: val_loss did not improve from 0.50318\n",
      "5400/5400 [==============================] - 328s 61ms/sample - loss: 0.5074 - accuracy: 0.7588 - val_loss: 0.5043 - val_accuracy: 0.7713\n",
      "Epoch 23/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.5019 - accuracy: 0.7655\n",
      "Epoch 00023: val_loss improved from 0.50318 to 0.50135, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 346s 64ms/sample - loss: 0.5019 - accuracy: 0.7654 - val_loss: 0.5014 - val_accuracy: 0.7746\n",
      "Epoch 24/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4969 - accuracy: 0.7667\n",
      "Epoch 00024: val_loss improved from 0.50135 to 0.49604, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 364s 67ms/sample - loss: 0.4971 - accuracy: 0.7666 - val_loss: 0.4960 - val_accuracy: 0.7746\n",
      "Epoch 25/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4974 - accuracy: 0.7644\n",
      "Epoch 00025: val_loss improved from 0.49604 to 0.49418, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 369s 68ms/sample - loss: 0.4974 - accuracy: 0.7643 - val_loss: 0.4942 - val_accuracy: 0.7729\n",
      "Epoch 26/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4960 - accuracy: 0.7669\n",
      "Epoch 00026: val_loss improved from 0.49418 to 0.49165, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 368s 68ms/sample - loss: 0.4954 - accuracy: 0.7673 - val_loss: 0.4916 - val_accuracy: 0.7708\n",
      "Epoch 27/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4904 - accuracy: 0.7713\n",
      "Epoch 00027: val_loss improved from 0.49165 to 0.49043, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 357s 66ms/sample - loss: 0.4903 - accuracy: 0.7714 - val_loss: 0.4904 - val_accuracy: 0.7758\n",
      "Epoch 28/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4856 - accuracy: 0.7715\n",
      "Epoch 00028: val_loss did not improve from 0.49043\n",
      "5400/5400 [==============================] - 327s 61ms/sample - loss: 0.4857 - accuracy: 0.7715 - val_loss: 0.4915 - val_accuracy: 0.7717\n",
      "Epoch 29/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4812 - accuracy: 0.7722\n",
      "Epoch 00029: val_loss improved from 0.49043 to 0.48817, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 330s 61ms/sample - loss: 0.4814 - accuracy: 0.7721 - val_loss: 0.4882 - val_accuracy: 0.7788\n",
      "Epoch 30/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4752 - accuracy: 0.7769\n",
      "Epoch 00030: val_loss improved from 0.48817 to 0.48687, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 361s 67ms/sample - loss: 0.4750 - accuracy: 0.7772 - val_loss: 0.4869 - val_accuracy: 0.7767\n",
      "Epoch 31/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4753 - accuracy: 0.7775\n",
      "Epoch 00031: val_loss did not improve from 0.48687\n",
      "5400/5400 [==============================] - 359s 66ms/sample - loss: 0.4753 - accuracy: 0.7775 - val_loss: 0.4879 - val_accuracy: 0.7783\n",
      "Epoch 32/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4707 - accuracy: 0.7819\n",
      "Epoch 00032: val_loss improved from 0.48687 to 0.48557, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 365s 68ms/sample - loss: 0.4706 - accuracy: 0.7819 - val_loss: 0.4856 - val_accuracy: 0.7783\n",
      "Epoch 33/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4669 - accuracy: 0.7839\n",
      "Epoch 00033: val_loss did not improve from 0.48557\n",
      "5400/5400 [==============================] - 361s 67ms/sample - loss: 0.4668 - accuracy: 0.7837 - val_loss: 0.4857 - val_accuracy: 0.7788\n",
      "Epoch 34/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4643 - accuracy: 0.7808\n",
      "Epoch 00034: val_loss improved from 0.48557 to 0.48331, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 349s 65ms/sample - loss: 0.4642 - accuracy: 0.7810 - val_loss: 0.4833 - val_accuracy: 0.7821\n",
      "Epoch 35/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4634 - accuracy: 0.7832\n",
      "Epoch 00035: val_loss did not improve from 0.48331\n",
      "5400/5400 [==============================] - 366s 68ms/sample - loss: 0.4634 - accuracy: 0.7832 - val_loss: 0.4848 - val_accuracy: 0.7738\n",
      "Epoch 36/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4567 - accuracy: 0.7883\n",
      "Epoch 00036: val_loss did not improve from 0.48331\n",
      "5400/5400 [==============================] - 368s 68ms/sample - loss: 0.4565 - accuracy: 0.7885 - val_loss: 0.4855 - val_accuracy: 0.7792\n",
      "Epoch 37/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4550 - accuracy: 0.7892\n",
      "Epoch 00037: val_loss did not improve from 0.48331\n",
      "5400/5400 [==============================] - 359s 67ms/sample - loss: 0.4551 - accuracy: 0.7890 - val_loss: 0.4844 - val_accuracy: 0.7808\n",
      "Epoch 38/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4523 - accuracy: 0.7879\n",
      "Epoch 00038: val_loss did not improve from 0.48331\n",
      "5400/5400 [==============================] - 335s 62ms/sample - loss: 0.4524 - accuracy: 0.7880 - val_loss: 0.4842 - val_accuracy: 0.7792\n",
      "Epoch 39/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4438 - accuracy: 0.7934\n",
      "Epoch 00039: val_loss did not improve from 0.48331\n",
      "5400/5400 [==============================] - 352s 65ms/sample - loss: 0.4441 - accuracy: 0.7933 - val_loss: 0.4840 - val_accuracy: 0.7738\n",
      "Epoch 40/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4480 - accuracy: 0.7902\n",
      "Epoch 00040: val_loss improved from 0.48331 to 0.48301, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 369s 68ms/sample - loss: 0.4478 - accuracy: 0.7905 - val_loss: 0.4830 - val_accuracy: 0.7783\n",
      "Epoch 41/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4429 - accuracy: 0.7931\n",
      "Epoch 00041: val_loss improved from 0.48301 to 0.48155, saving model to best_model.hdf5\n",
      "5400/5400 [==============================] - 346s 64ms/sample - loss: 0.4426 - accuracy: 0.7933 - val_loss: 0.4815 - val_accuracy: 0.7771\n",
      "Epoch 42/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4382 - accuracy: 0.7991\n",
      "Epoch 00042: val_loss did not improve from 0.48155\n",
      "5400/5400 [==============================] - 343s 64ms/sample - loss: 0.4383 - accuracy: 0.7992 - val_loss: 0.4844 - val_accuracy: 0.7779\n",
      "Epoch 43/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4349 - accuracy: 0.7978\n",
      "Epoch 00043: val_loss did not improve from 0.48155\n",
      "5400/5400 [==============================] - 346s 64ms/sample - loss: 0.4347 - accuracy: 0.7980 - val_loss: 0.4837 - val_accuracy: 0.7808\n",
      "Epoch 44/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4281 - accuracy: 0.8010\n",
      "Epoch 00044: val_loss did not improve from 0.48155\n",
      "5400/5400 [==============================] - 353s 65ms/sample - loss: 0.4282 - accuracy: 0.8008 - val_loss: 0.4847 - val_accuracy: 0.7829\n",
      "Epoch 45/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4287 - accuracy: 0.8006\n",
      "Epoch 00045: val_loss did not improve from 0.48155\n",
      "5400/5400 [==============================] - 351s 65ms/sample - loss: 0.4291 - accuracy: 0.8002 - val_loss: 0.4884 - val_accuracy: 0.7775\n",
      "Epoch 46/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4251 - accuracy: 0.8012\n",
      "Epoch 00046: val_loss did not improve from 0.48155\n",
      "5400/5400 [==============================] - 352s 65ms/sample - loss: 0.4254 - accuracy: 0.8011 - val_loss: 0.4886 - val_accuracy: 0.7717\n",
      "Epoch 47/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4206 - accuracy: 0.8029\n",
      "Epoch 00047: val_loss did not improve from 0.48155\n",
      "5400/5400 [==============================] - 346s 64ms/sample - loss: 0.4208 - accuracy: 0.8028 - val_loss: 0.4875 - val_accuracy: 0.7738\n",
      "Epoch 48/200\n",
      "5376/5400 [============================>.] - ETA: 1s - loss: 0.4198 - accuracy: 0.8020\n",
      "Epoch 00048: val_loss did not improve from 0.48155\n",
      "5400/5400 [==============================] - 341s 63ms/sample - loss: 0.4202 - accuracy: 0.8018 - val_loss: 0.4840 - val_accuracy: 0.7792\n",
      "Epoch 49/200\n",
      "4864/5400 [==========================>...] - ETA: 34s - loss: 0.4114 - accuracy: 0.8056"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1208 01:00:51.178490 4670715328 callbacks.py:990] Can save best model only with val_loss available, skipping.\n",
      "W1208 01:00:51.222038 4670715328 callbacks.py:1250] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "4864/5400 [==========================>...] - ETA: 34s - loss: 0.4114 - accuracy: 0.8056"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-96d697552d8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspatial_dr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-66-182e05d698fc>\u001b[0m in \u001b[0;36mbuild_model2\u001b[0;34m(lr, lr_d, units, spatial_dr, kernel_size1, kernel_size2, dense_units, dr, conv_size)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"binary_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabstract\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'THEORETICAL'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ENGINEERING'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'EMPIRICAL'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'OTHERS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheck_point\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model3 = build_model2(lr = 1e-4, lr_d = 0, units = 512, spatial_dr = 0.5, kernel_size1=4, kernel_size2=3, dense_units=32, dr=0.1, conv_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1000/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 15s 15ms/sample\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model1 = load_model('/Users/zackary/Desktop/comp/paper/600_best_model.h5')\n",
    "pred1 = model1.predict(abstract[6000:7000], batch_size = 1024, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1000/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 18s 18ms/sample\n"
     ]
    }
   ],
   "source": [
    "model2 = load_model('/Users/zackary/Desktop/comp/paper/600_noCon_best_model.h5')\n",
    "pred2 = model2.predict(abstract[6000:7000], batch_size = 1024, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAADPCAYAAAD70BriAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAHgFJREFUeJzt3X9sHOd95/H316QpkSdYJi2KUG2FtGTZps85JJEaB2dUzg+5pY1zbaC+xG4vMXIJeLiL74/cP3LhC4xcUcC6f3I2GiRHBG7sOyRproATBXVl2FZqAUKdmkxTWw6lmGLESi4j0qKOPnZJSUt/74+dGT872tWS3F3ucvh5AcTOPPPM83znmdmvRrOzO+buiIhIdl3V6ABERKS+lOhFRDJOiV5EJOOU6EVEMk6JXkQk45ToRUQyToleRCTjlOhFRDJOiV5EJONaGx0AwJYtW7yvr6/RYYiIrCkjIyPvunt3pXpNkej7+voYHh5udBgiImuKmU0spV5TJPr16Hs/+8cVr/uHd3yohpGISNYp0VehmmQtIrJa9GGsiEjGKdGLiGScEr2ISMYp0YuIZJwSvYhIxinRi4hknBK9iEjGKdGLiGRcxURvZs+Y2ZSZHQvKuszsJTN7O3rtjMrNzJ42szEze8PMPlbP4EVEpLKlnNF/FxhIlT0GvOLuu4BXonmAe4Bd0d8g8K3ahCkiIitVMdG7+xFgJlV8P/BsNP0s8EBQ/pwXvAZca2bbahWsiIgs30p/66bH3Sej6d8APdH09cDpoN6ZqGySJqXfqxGRrKv6w1h3d8CXu56ZDZrZsJkNT09PVxuGiIiUsdJEfza+JBO9TkXl7wDbg3o3RGWXcfchd9/j7nu6uyv+br6IiKzQSi/dHAQeAZ6MXn8clD9qZj8A7gBmg0s8UiP6LXsRWY6Kid7Mvg98EthiZmeAJygk+B+a2ZeACeCzUfUXgHuBMSAHfLEOMYuIyDJUTPTu/nCZRZ8pUdeBr1QblIiI1I6+GSsiknFK9CIiGadnxq4z1X5vQB/miqw9OqMXEck4JXoRkYxTohcRyTglehGRjNOHsSIiVVgLNzgo0YvIupf1X7FVopdladQbQrd1iqycEr2I1JROBpqPEr1kXiN/7VO/NLp6sn75pRpK9LImrMc38XrcZqmPuiR6MxsAngJagO+4+5P16Af0ZpD60vElWVDz++jNrAX4JnAPcBvwsJndVut+RERkaepxRv9xYMzdxwGip03dD/yyDn3x8uhZ9vX3lC1/efQsIxMzdHa0saN7E+PTcwzu3cmBQ6MsXFoEYOPVLXR2tHHqXI4NrcaFvGNc/iDcFoPFVOGV6ocqLV+uDa0GwIW8J/MX805bEM817a28N5/nKoP3vTA/O5+n77oOJs7lkng2t7cyt5Bn0aHvug4mZ+e586ZuxqfnmDiX45r2wmEyt5Dnrlu28uqJKTZtbGV3bxevnpiitcXYeHUL+wf6OXBolP0D/SXHfeJcjrYo7jtv6k6WA5yeybFpYyv7B/oZOnKSwb07eXn0bNH+ituK9+v49ByTs/Ns29yelA8dOcmO7k3JOI1MzLC7t4vx6bkkjrjugUOjyTKAydl5nrjv9qSNMP64z7gPgPO5i+zu7Ur6iuvExqfniurE6w7u3cnQkZNF/QHJdu7u7eLo2HRyXMZjEa4fz6fHZejISU6dy7G5vTUZ27jdeL+E4xdvRzjucTyxeCzC7Yi3MxyPsH5Y79UTU9x1y9Zk7NPz8bYuXFpM9mUcW9jeyMQMC5cWk2Mn3HfxNsTjF/cR74e4jXg6Xv/o2HTS59GxwrOrt21uT/oe3LuTr/3ozeS4v+uWrYxMzAAUHdvncxeT98imja3Ja1jv6Ng0d97UnfQZb9vZ9xb46t03U0/1SPTXA6eD+TMUHitYF4ePT5VM9HH54eOFx9nOzuc5dS6XLJ+dzyfTF/L5ZD5OnKWScjrJV6ofqmWSD/tNz4fxxNsUxx3Ph+MQlofL4nFLLw/HM55ezDsX8vmiuuXGPY4vXJ7up1wMcVvhfo3rx+XxdCiuG5fHdcNtCIVtpPuM+0i3DVwWV7pOuG7YRvq4jOuHx2W673g+PS5h3KXGNv2+SG93Op5wPtyOUtuUrh/WS++D9Hx8/JTaf+X2Z6ltCMcvfYyU2550n+n+Fr10n+ljOyxPv5bb7lPnchw+PlX3RG+Fh0LVsEGzB4EBd/9yNP954A53fzRVbxAYjGZvAU7UNJDKtgDvrnKfa43GqDKN0ZVpfCqrZox63b27UqV6nNG/A2wP5m+Iyoq4+xAwVIf+l8TMht19T6P6Xws0RpVpjK5M41PZaoxRPX7U7HVgl5ndaGZtwEPAwTr0IyIiS1DzM3p3z5vZo8CLFG6vfMbd36p1PyIisjR1uY/e3V8AXqhH2zXUsMtGa4jGqDKN0ZVpfCqr+xjV/MNYERFpLnrwiIhIxjXFb91s2bLF+/r6Gh2GiMiaMjIy8m6jbq9ctr6+PoaHhxsdhojImmJmE0up1xSJXkSkWvpJ6PKU6EWkaejXQutDH8aKiGScEr2ISMYp0YuIZJwSvYhIxinRi4hknBK9iEjGKdGLiGScEr2ISMZVTPRm9oyZTZnZsaCsy8xeMrO3o9fOqNzM7GkzGzOzN8zsY/UMXkREKlvKN2O/C/wZ8FxQ9hjwirs/aWaPRfP7gXuAXdHfHcC3qOODwUWk+ejbrc2n4hm9ux8BZlLF9wPPRtPPAg8E5c95wWvAtWa2rVbBiojI8q30Gn2Pu09G078BeqLp64HTQb0zUZmIiDRI1R/GeuERVct+TJWZDZrZsJkNT09PVxuGiIiUsdJEfza+JBO9TkXl7wDbg3o3RGWXcfchd9/j7nu6uyv+br6IiKzQSn+m+CDwCPBk9PrjoPxRM/sBhQ9hZ4NLPCKyBujD1OypmOjN7PvAJ4EtZnYGeIJCgv+hmX0JmAA+G1V/AbgXGANywBfrELOIiCxDxUTv7g+XWfSZEnUd+Eq1QYmISO3oCVMisu5l/TGE+gkEEZGMU6IXEck4JXoRkYxTohcRyTglehGRjNNdNyIZpC89SUhn9CIiGadELyKScbp00yBZ/4KGiDQPJXpZNevxHzddK5dmsO4T/Vp8IzYyYTZqvNbifhJpFus+0a83Spgi609dEr2ZDQBPAS3Ad9z9yXr0A0pcIiKV1DzRm1kL8E3gbgrPjH3dzA66+y9r3ZeISKNVe7K5Gp8/1eOM/uPAmLuPA0RPm7ofqEuif3n0LAD7+nuS+X39PUn5+PQcg3t3JstGJmbY3dvFvv4eDhwaZf9APy+PnmV8eg6A87mL7O7tStY9n7vI/oF+ho6cZEf3pqT89EyOTRs/GL79A/0cODSarBvHM3TkJIN7d/K1H73JXbdsBeDo2DR33tSdtLO9qyPpe+HSInfeVPxoxfHpOSZn59l4dQudHW0M7t2ZbB/AyMQMnR1t7OjexMjETNJGvO1xDI8//yafunVrsq3xsni7wnbi+f0D/QAcODRKZ0db0XgA7OjexPj0XNHYDO7dydd/coxtm9uT7QrHOY6zs6MtiSNsf+HSIk/cd3sSW7zP4tdwv4T7OWz3fO5isl/idkrFOXTkJKdncvzJAx8uaisWtwnQ2dHG5Ox8sl3pZeFxlj4mQ0fHpovaiL16YortXR1J7AuXFtl4dctl2zk5O58cI/H2TpzLJfs2HpsDh0YBisYsbhcgv+hs7+rg9EwO4LJpgFPnckl8LQaLDhtajfyisxg9KXpzeyuz83k2tBoX84XC+CHSRvEDpVsMWluMC/nLHzO9odUASi6rxobW0v2FMS36B7G2FMLgff8g9hYj2TfxcRsfC6dncsl7e19/T/J+A5L3SaljI85V33jpV3z17ptrus1p9Uj01wOng/kzFB4rWBeHjxceVxu/qQ4fn2Jff09SXqpuXGd2Pl9Unq4XOnUuV3TQA8n64Xw6nnidRS9uN5xOt1uqf4AL+XzZmGfn80XtlGrfS6yX3q50O2F5uL1xnfTrB7F62e2Ky8P20u2nYwv3Xak2y7WbbicdU7kxK7U8brdUW2GfpY7Jcu1eKR4o7PNSbYTzVzomyvVfqr9y07E4saeTZtxPqWSaLll0WCyTdGud4JfabrxdnppP1yl1fKWPz339PVccx7BenIeeeuXtuid6KzwUqoYNmj0IDLj7l6P5zwN3uPujqXqDwGA0ewtwYhndbAHerUG49aY4a2+txKo4a0txltbr7t2VKtXjjP4dYHswf0NUVsTdh4ChlXRgZsPuvmdl4a0exVl7ayVWxVlbirM69fgJhNeBXWZ2o5m1AQ8BB+vQj4iILEHNz+jdPW9mjwIvUri98hl3f6vW/YiIyNJUPKM3s2fMbMrMjgVlXWb2kpm9Hb12RuVmZk8DTwMLwL919z+tQ9wruuTTAIqz9tZKrIqzthRnFSp+GGtme4E54Dl3vz0q++/AjLs/aWaPAZ3uvt/M7gX+M3AvhTttnnL3ut1xIyIilVU8o3f3I8BMqvh+4Nlo+lnggaD8OS94DbjWzLbVKlgREVm+lV6j73H3yWj6N0BPNF3qHvrrgUmuYMuWLd7X17fCUERE1qeRkZF3V+X2Snd3M1v2zfjhffQf+tCHGB4erjYUEZF1xcwmllJvpYn+rJltc/fJ6NJM/NW7Jd1DD8X30e/Zs6c+X4kTWafW42//S3krvY/+IPBINP0I8OOg/AvR3TefAGaDSzwiItIAFc/ozez7wCeBLWZ2BngCeBL4oZl9CZgAPhtVf4HCHTdjQA74Yh1iFhGRZaiY6N394TKLPlOirgNfqTYoERGpnXr8BIKIiDQRPUpQRGpKHwQ3HyV6ESmix3NmjxK9SJNSwpVa0TV6EZGMU6IXEck4JXoRkYxTohcRyTh9GCsiTUO3ZtaHzuhFRDJOiV5EJON06UbkCnQvu2SBEr1knpK1rHdK9OtMtUlPH3hJs9IHueUp0cuy6M0ksvYo0cuq0T8SIo2hu25ERDJOZ/RVaNSHfDq7FZHl0Bm9iEjGrfszet16tzZoP4ms3LpP9GuRkp6ILIcu3YiIZJwSvYhIxq35Sze6jCEicmVrPtGLiFQr61/m06UbEZGMU6IXEck4JXoRkYyrS6I3swEzO2FmY2b2WD36EBGRpal5ojezFuCbwD3AbcDDZnZbrfsREZGlqcddNx8Hxtx9HMDMfgDcD/yyDn3x8ujZZHpffw9DR04CsKN7E/v6ezhwaJTdvV2MT88xOTvPnTd1AzA+PZfUG5+e43zuIgALlxbJLzqbNn4wNPsH+gEYOnIyqR+3NTIxQ2dHG4N7d3Lg0ChA0t/pmRzbuzqSuq+emGLTxlZ293YxMjGT1Dufu8jCpcUktjjuzo42Js7laGs1Nl7dwtxCnu1dHUn9bZvb2dG9CaAojq//5Bh33tSdxBBvy/6Bfg4cGuW9+Txtrca2ze1Mzs5zMe8A9F7XkcQ8uHdnMpZxm6Vi2Hh1C+/N5+m9rgOgqL/dvV28emIqqR/3H4/x9q4OdnRvSsbip8enuMrgrlu2cnRsmifuu53Hn3+Ta9oLYwZw+PgUn751a9E+B5Lxisf66Ng0+UXnTx74MF/70Zts7yps2123bGVkYgag5Hiez11Mxie/6LzvcE17YXs6O9qS7YvrA5zPXWRuIZ/UfW8+zzXtrcwt5AGK6p/PXWR3bxdHx6bZtrmdU+dyGODAhmA/x2NwMe840GKF7V10sOD4772ug4lzOYCkz3j8Z+cL/bdYIYbZ+Tx9QX2P2tgcrfe+Q1urcTHvXNPemhx/HvS3IRqXsI8N0bEU17UorlPncrQYvB81EC+DQj8AF/JOS7TP08drZ0cbp87l6IuOLYBTUezxNi1cWgRg49UtSbyfunUrh49PJX3Hx3W878P339Gx6eR9HB+z8ftpd28X+/p7inLMyMQM+wf6eXn0bLIsXh7Px8dkWrxs6MhJBvfuTMrPvrfAV+++ueQ6tVKPRH89cDqYPwPcUYd+gMIbP7avvyc5EE6dy7Gvv4fZ+XxRnXA6rldK/CZJ1w3rx23FdePXsI+4flg3nk7HEs/HccftXcg7F/L5y+JNxxPWD9sOtyWsk972cOzSfZWLoVRZehxKLUvHH9dd9OJxcbjiPozfVOF4pcd10S/fD+ltLrW96ZjTr6WOnXSdUvXT4xL3FI5xqW2IhYm31Nimj91FX1rMcQxxWan3QLg8LAvb9aCfMO4w9nB80/t8KeMcblOhvcv3f9x3etzLvZaa3tffc9m+KLcsni+X6ONl6e156pW3657ozd0r11pOg2YPAgPu/uVo/vPAHe7+aKreIDAYzd4CnFhmV1uAd6sMt5lkaXuytC2Qre3J0rZAtrZnJdvS6+7dlSrV44z+HWB7MH9DVFbE3YeAoZV2YmbD7r5npes3myxtT5a2BbK1PVnaFsjW9tRzW+px183rwC4zu9HM2oCHgIN16EdERJag5mf07p43s0eBF4EW4Bl3f6vW/YiIyNLU5bdu3P0F4IV6tB1Y8WWfJpWl7cnStkC2tidL2wLZ2p66bUvNP4wVEZHmop9AEBHJuKb4meItW7Z4X19fo8MQEVlTRkZG3m3U7ZXL1tfXx/DwcKPDEBFZU8xsYin1miLRy9rRqCd6rYWHO4g0K12jFxHJOCV6EZGMU6IXEck4JXoRkYxTohcRyTglehGRjFOiFxHJOCV6EZGMq5jozewZM5sys2NBWZeZvWRmb0evnVG5mdnTZjZmZm+Y2cfqGbyIiFS2lDP67wIDqbLHgFfcfRfwSjQPcA+wK/obBL5VmzBFRGSlKiZ6dz8CzKSK7weejaafBR4Iyp/zgteAa81sW62CFRGR5Vvpb930uPtkNP0bIH7s+fXA6aDemahsEmkKjfqtGhFpnKo/jPXCk0uW/fQSMxs0s2EzG56enq42DBERKWOlZ/RnzWybu09Gl2amovJ3gO1BvRuissu4+xDRo7P27Nmjx1zJFVXzPxH98qWsdys9oz8IPBJNPwL8OCj/QnT3zSeA2eASj4iINEDFM3oz+z7wSWCLmZ0BngCeBH5oZl8CJoDPRtVfAO4FxoAc8MU6xCwiIstQMdG7+8NlFn2mRF0HvlJtUCIiUjv6ZqyISMYp0YuIZJyeGSuZpzt2ZL3TGb2ISMat+TP6ar/puRbP2PTtVhFZDp3Ri4hknBK9iEjGKdGLiGScEr2ISMat+Q9jq6Vb70Qk69Z9oq+G7n7Jvkbe1aWTEKkVJXqROtLJgDQDJXqRDGrkPzD630Tz0YexIiIZV5czejMbAJ4CWoDvuPuT9ehHRJqPPltoPjVP9GbWAnwTuJvCw8FfN7OD7v7LWvclItmifyTqox5n9B8Hxtx9HMDMfgDcD9Ql0R84NEpnRxunzuXY3N6aTG9oNfKLzqaNhU18bz7Pp27dysjEDAuXFtl4dQtzC3laW4w7b+pmfHqO0zM5Nm1sZeHSIts2t3N6JgfA9q4OdnRvYl9/T1G/u3u7kvbuvKmbkYkZdvd2MT49x47uTYxPzyX1z+cu0tnRdtn84N6dvDx6ln39PRw4NArA7t4ujo5Ns/HqlqTO139yjAt559O3bmVffw+PP/8mvdd1FMX8xH23J/X6rutgcnY+Wefw8SlarND3Yokn9G5oNS7kCwtarLDNE+cK23+VfbCOUfwk+PT8Sm1ub2V2Pl+xXrhdpdoAmJ3Ps7m9lbmFPIteiPFTt27lp8enuMqgNRqIi3nHo/XmFj7oe9EL/Qzu3cmBQ6PJ8RLr7GjjfO5i0f7ftrmd87mL7B/o5+XRs0VxjU/PMTk7D8AT993OgUOj7B/oB+DrPznGts3tSd34uNnRvYmRiRkA9g/08/WfHEuOh7hO3N/XfvQm27s6ivqMj684zlB4nE7OzrNtczs7ujclscbH5MjETHLMhu2Edcod6/E4xMuBovdP3H5cb19/Dy+PnuXVE1PcdcvWpG44lmG/4fJ43TjRf+OlXwHw1btv5hsv/Yqv3n0zpZRbFq6f9rn/+bd8Ysd1Rcu+97N/LIozHRvA0JGTDO7dWTKOs+8tlI2xVuqR6K8HTgfzZ4A76tAPUHhTxwkinI4TQZg8Dh+fSqYv5Avli3kvKo/rn4qSXDx96lyu6ECdnc8XrRdPx6/h+um20/OHj0+xr7+naD6OMb09cV0P+gjbjeuF/cftlUrw6fXieuH64XrpJmr1VPelJHkoPa6l2ginneIxWEz9I1Gq7/TYxsdLWBbu/1LjvZQ4L+T9smMt3V5cLz4e0svS++tKcZaKMT6+Sy0v9/4J58vtk/Ty8P2Tfu/s6+8peg/FdcvFHi6P143/N/DUK28D0HPNRp565W16rtlYMr5yy8L103726xl+9uuZy5aFcaZjC8egXBz1TvRWePpfDRs0exAYcPcvR/OfB+5w90dT9QaBwWj2FuDECrvcAry7wnVXi2KsXrPHB4qxFpo9PmiuGHvdvbtSpXqc0b8DbA/mb4jKirj7EDBUbWdmNuzue6ptp54UY/WaPT5QjLXQ7PHB2ogxrR63V74O7DKzG82sDXgIOFiHfkREZAlqfkbv7nkzexR4kcLtlc+4+1u17kdERJamLvfRu/sLwAv1aLuEqi//rALFWL1mjw8UYy00e3ywNmIsUvMPY0VEpLnoJxBERDKu6RK9mQ2Y2QkzGzOzx0os32BmfxEt/5mZ9QXL/jgqP2Fmv7fUNlcjPjO728xGzOzN6PXTwTp/E7X5i+hva4Ni7DOz+SCObwfr7I5iHzOzp83MGhTjHwXx/cLM3jezj0TLajaOS4hvr5n93Mzy0S3F4bJHzOzt6O+RoHy1x7BkjGb2ETP7WzN7y8zeMLPPBcu+a2a/DsbwI42IMVq2GMRxMCi/MTomxqJjpC3dbr3jM7NPpY7DBTN7IFpW0zGsCXdvmj8KH96eBHYAbcA/ALel6vwn4NvR9EPAX0TTt0X1NwA3Ru20LKXNVYrvo8BvRdO3A+8E6/wNsKcJxrAPOFam3b8DPkHhi6Z/DdzTiBhTdT4MnKz1OC4xvj7gXwHPAQ8G5V3AePTaGU13NmgMy8V4M7Armv4tYBK4Npr/bli3UeMYLZsr0+4PgYei6W8D/7ER8aX2+QzQUesxrNVfs53RJz+f4O4XgfjnE0L3A89G038JfCY6M7of+IG7X3D3XwNjUXtLabPu8bn737v7P0XlbwHtZrZhhXHUJcZyDZrZNuAad3/NC0fyc8ADTRDjw9G6tVYxPnc/5e5vAO+n1v094CV3n3H388BLwEAjxrBcjO7+K3d/O5r+J2AKqPilm9WMsZzoGPg0hWMCCsfISsexVvE9CPy1u5f/+muDNVuiL/XzCdeXq+PueWAWuO4K6y6lzdWIL/QHwM/d/UJQ9ufRf/O+VuV/6auN8UYz+3sze9XMfieof6ZCm6sZY+xzwPdTZbUYx2qOmSsdh6s9hhWZ2ccpnM2eDIr/NLqk840qT0aqjXGjmQ2b2WvxZREKx8D/jY6JlbRZy/hiD3H5cVirMayJZkv0mWdm/xI4APyHoPiP3P3DwO9Ef59vRGwU/gv/IXf/KPBfgO+Z2TUNiuWKzOwOIOfux4LiZhnHNSH6X8b/Ar7o7vEZ6x8DtwK/TeGSxP4GhQeFr/fvAf4Q+B9mVvpXwRooGsMPU/jeUKyZxhBovkS/lJ9PSOqYWSuwGTh3hXWX9JMMqxAfZnYD8DzwBXdPzqDc/Z3o9f8B36PwX8qVWnGM0WWvc1EsIxTO8m6O6t9Qoc1ViTFYftlZVA3HsZpj5krH4WqPYVnRP+B/BTzu7q/F5e4+6QUXgD+n/sdiWcH+HKfw+ctHKRwD10bHxLLbrGV8kc8Cz7v7pbigxmNYE82W6Jfy8wkHgfhOhgeBw9E1z4PAQ1a4W+NGYBeFD79q+ZMMK47PzK6l8MZ6zN2PxpXNrNXMtkTTVwP/BjjGylUTY7cVnieAme2gMIbj7j4JvGdmn4guh3wB+HEjYoxiu4rCGyy5Pl/jcazmmHkR+F0z6zSzTuB3gRcbNIYlRfWfB55z979MLdsWvRqFa9/1PhbLxdgZX/KI9uudwC+jY+CnFI4JKBwjKx3HWuSGh0mdcNR4DGuj0Z8Gp/+Ae4FfUTibfDwq+2/A70fTG4H/Q+HD1r8DdgTrPh6td4LgjoZSba52fMB/Bf4Z+EXwtxX4F8AI8AaFD2mfAloaFOMfRDH8Avg5cF/Q5h4KB+xJ4M+IvmzXoP38SeC1VHs1HcclxPfbFK7p/jOFs8y3gnX/fRT3GIXLIo0aw5IxAv8OuJQ6Fj8SLTsMvBnF+b+BTQ2K8V9HcfxD9PqloM0d0TExFh0jGxq0n/so/A/gqlSbNR3DWvzpm7EiIhnXbJduRESkxpToRUQyToleRCTjlOhFRDJOiV5EJOOU6EVEMk6JXkQk45ToRUQy7v8DVqjFkxkpjjcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = np.array(list(pred2))\n",
    "a = y[:,0]\n",
    "b = y[:,1]\n",
    "c = y[:,2]\n",
    "d = y[:,3]\n",
    "\n",
    "import seaborn as sns\n",
    "plt.subplot(5,1,1)\n",
    "sns.distplot(a, bins=20, kde=False, rug=True);\n",
    "plt.subplot(5,1,2)\n",
    "sns.distplot(b, bins=20, kde=False, rug=True);\n",
    "plt.subplot(5,1,3)\n",
    "sns.distplot(c, bins=20, kde=False, rug=True);\n",
    "plt.subplot(5,1,4)\n",
    "sns.distplot(d, bins=20, kde=False, rug=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6849037487335359 0.34\n",
      "0.7143911439114392 0.23\n",
      "0.5140997830802603 0.29\n",
      "0.10218978102189781 0.12\n"
     ]
    }
   ],
   "source": [
    "#pred = pred3 + pred4 + pred5\n",
    "th1 = 0\n",
    "th2 = 0\n",
    "th3 = 0\n",
    "th4 = 0\n",
    "max1 = 0\n",
    "max2 = 0\n",
    "max3 = 0\n",
    "max4 = 0\n",
    "for thre in range(1000):\n",
    "    y = np.array(list(pred2))\n",
    "    a = y[:,0]\n",
    "    b = y[:,1]\n",
    "    c = y[:,2]\n",
    "    d = y[:,3]\n",
    "    for i in range(1000):\n",
    "        if a[i] > thre*0.01:\n",
    "            a[i] = 1\n",
    "        else:\n",
    "            a[i] = 0\n",
    "        if b[i] > thre*0.01:\n",
    "            b[i] = 1\n",
    "        else:\n",
    "            b[i] = 0\n",
    "        if c[i] > thre*0.01:\n",
    "            c[i] = 1\n",
    "        else:\n",
    "            c[i] = 0\n",
    "        if d[i] > thre*0.01:\n",
    "            d[i] = 1\n",
    "        else:\n",
    "            d[i] = 0\n",
    "    if max1 < f1_score(a, data['THEORETICAL'][6000:7000]):\n",
    "        max1 = f1_score(a, data['THEORETICAL'][6000:7000])\n",
    "        th1 = thre*0.01\n",
    "    if max2 < f1_score(b, data['ENGINEERING'][6000:7000]):\n",
    "        max2 = f1_score(b, data['ENGINEERING'][6000:7000])\n",
    "        th2 = thre*0.01\n",
    "    if max3 < f1_score(c, data['EMPIRICAL'][6000:7000]):\n",
    "        max3 = f1_score(c, data['EMPIRICAL'][6000:7000])\n",
    "        th3 = thre*0.01\n",
    "    if max4 < f1_score(d, data['OTHERS'][6000:7000]):\n",
    "        max4 = f1_score(d, data['OTHERS'][6000:7000])\n",
    "        th4 = thre*0.01\n",
    "                       \n",
    "print(max1, th1)\n",
    "print(max2, th2)\n",
    "print(max3, th3)\n",
    "print(max4, th4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(list(pred1))\n",
    "a = y[:,0]\n",
    "b = y[:,1]\n",
    "c = y[:,2]\n",
    "d = y[:,3]\n",
    "\n",
    "for i in range(20000):\n",
    "    if a[i] > 0.38:\n",
    "        a[i] = 1\n",
    "    else:\n",
    "        a[i] = 0\n",
    "    if b[i] > 0.31:\n",
    "        b[i] = 1\n",
    "    else:\n",
    "        b[i] = 0\n",
    "    if c[i] > 0.33:\n",
    "        c[i] = 1\n",
    "    else:\n",
    "        c[i] = 0\n",
    "    if d[i] > 0.28:\n",
    "        d[i] = 1\n",
    "    else:\n",
    "        d[i] = 0\n",
    "#pre = np.append(a, [b, c, d])\n",
    "#org = np.append(data[6000:7000]['THEORETICAL'],[data[6000:7000]['ENGINEERING'],data[6000:7000]['EMPIRICAL'],data[6000:7000]['OTHERS']])\n",
    "#f1_score(pre, org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 0.7253668763102725 0.38\n",
    "* 0.7604501607717042 0.31\n",
    "* 0.5900383141762453 0.33\n",
    "* 0.34375000000000006 0.28\n",
    "* 0.69688013136289\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame()\n",
    "output = pd.read_csv('task2_sample_submission.csv', encoding = \"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.58155, 0.67815, 0.36875, 0.02975)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"THEORETICAL\"][:20000].mean(), output[\"ENGINEERING\"][:20000].mean(), output[\"EMPIRICAL\"][:20000].mean(), output[\"OTHERS\"][:20000].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "output[\"THEORETICAL\"][:20000] = a\n",
    "output[\"ENGINEERING\"][:20000] = b\n",
    "output[\"EMPIRICAL\"][:20000] = c\n",
    "output[\"OTHERS\"][:20000] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>THEORETICAL</th>\n",
       "      <th>ENGINEERING</th>\n",
       "      <th>EMPIRICAL</th>\n",
       "      <th>OTHERS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T00001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T00002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T00003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T00004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T00005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T00006</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>T00007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>T00008</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>T00009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>T00010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  order_id  THEORETICAL  ENGINEERING  EMPIRICAL  OTHERS\n",
       "0   T00001          0.0          1.0        1.0     0.0\n",
       "1   T00002          0.0          1.0        0.0     0.0\n",
       "2   T00003          0.0          1.0        0.0     0.0\n",
       "3   T00004          1.0          0.0        0.0     0.0\n",
       "4   T00005          1.0          0.0        0.0     0.0\n",
       "5   T00006          1.0          0.0        0.0     0.0\n",
       "6   T00007          0.0          1.0        1.0     0.0\n",
       "7   T00008          1.0          1.0        0.0     0.0\n",
       "8   T00009          0.0          0.0        1.0     0.0\n",
       "9   T00010          1.0          1.0        0.0     0.0"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\"submit.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 3)"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[[0, 1, 2], [1, 2, 3], [2, 4, 5]]])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "      <th>THEORETICAL</th>\n",
       "      <th>ENGINEERING</th>\n",
       "      <th>EMPIRICAL</th>\n",
       "      <th>OTHERS</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rapid popularity of Internet of Things (IoT) a...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A Brain-Inspired Trust Management Model to Ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In this paper, we address the problem of compu...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>On Efficient Computation of Shortest Dubins Pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>High quality upsampling of sparse 3D point clo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Data-driven Upsampling of Point Clouds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Internet is the main source of information now...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Accessibility or Usability of InteractSE? A He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Automated Facial Expression Recognition (FER) ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Spatio-Temporal Facial Expression Recognition ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Abstract  THEORETICAL  \\\n",
       "0  Rapid popularity of Internet of Things (IoT) a...          1.0   \n",
       "1  In this paper, we address the problem of compu...          1.0   \n",
       "2  High quality upsampling of sparse 3D point clo...          0.0   \n",
       "3  Internet is the main source of information now...          0.0   \n",
       "4  Automated Facial Expression Recognition (FER) ...          0.0   \n",
       "\n",
       "   ENGINEERING  EMPIRICAL  OTHERS  \\\n",
       "0          0.0        0.0     0.0   \n",
       "1          0.0        0.0     0.0   \n",
       "2          1.0        0.0     0.0   \n",
       "3          0.0        1.0     0.0   \n",
       "4          1.0        0.0     0.0   \n",
       "\n",
       "                                               Title  \n",
       "0  A Brain-Inspired Trust Management Model to Ass...  \n",
       "1  On Efficient Computation of Shortest Dubins Pa...  \n",
       "2             Data-driven Upsampling of Point Clouds  \n",
       "3  Accessibility or Usability of InteractSE? A He...  \n",
       "4  Spatio-Temporal Facial Expression Recognition ...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(pd.read_csv('data.csv', encoding = \"utf8\"))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "abstract = data['Abstract']\n",
    "#abstract = abstract.str.lower()\n",
    "for i in range(len(abstract)):\n",
    "  str = abstract[i]\n",
    "  abstract[i] = str.replace('$$$', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame()\n",
    "output['text'] = texts\n",
    "output['THEORETICAL'] = data['THEORETICAL']\n",
    "output['ENGINEERING'] = data['ENGINEERING']\n",
    "output['EMPIRICAL'] = data['EMPIRICAL']\n",
    "output['OTHERS'] = data['OTHERS']\n",
    "#data[['Abstract', \"THEORETICAL\", \"ENGINEERING\", \"EMPIRICAL\", \"OTHERS\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>THEORETICAL</th>\n",
       "      <th>ENGINEERING</th>\n",
       "      <th>EMPIRICAL</th>\n",
       "      <th>OTHERS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rapid popularity internet things ( iot ) cloud...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>paper , address problem computing optimal path...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>high quality upsampling sparse 3d point clouds...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>internet main source information nowadays.\\nse...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>automated facial expression recognition ( fer ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  THEORETICAL  \\\n",
       "0  rapid popularity internet things ( iot ) cloud...          1.0   \n",
       "1  paper , address problem computing optimal path...          1.0   \n",
       "2  high quality upsampling sparse 3d point clouds...          0.0   \n",
       "3  internet main source information nowadays.\\nse...          0.0   \n",
       "4  automated facial expression recognition ( fer ...          0.0   \n",
       "\n",
       "   ENGINEERING  EMPIRICAL  OTHERS  \n",
       "0          0.0        0.0     0.0  \n",
       "1          0.0        0.0     0.0  \n",
       "2          1.0        0.0     0.0  \n",
       "3          0.0        1.0     0.0  \n",
       "4          1.0        0.0     0.0  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[:6000].to_csv(\"train.csv\")\n",
    "output[6000:7000].to_csv(\"val.csv\")\n",
    "output[7000:].to_csv(\"sub.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
